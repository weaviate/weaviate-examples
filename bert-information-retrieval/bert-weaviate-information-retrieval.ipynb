{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install transformers\n",
    "# !pip3 install nltk\n",
    "# !pip3 install torch\n",
    "# !pip3 install weaviate-client==3.2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the BERT transformer model and pytorch\n",
    "\n",
    "We are using the `bert-base-uncased` model in this example, but any model will work. Feel free to adjust accordingly.\n",
    "\n",
    "## Initialize Weaviate Client\n",
    "This assumes you have Weaviate running locally on `:8080`. Adjust URL accordingly. You could also enter the WCS URL here, for example, if you are running a WCS cloud instance instead of running Weaviate locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/stefanbogdan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import weaviate\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# udpated to use different model if desired\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model.to('cuda') # remove if working without GPUs\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# initialize nltk (for tokenizing sentences)\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# initialize weaviate client for importing and searching\n",
    "client = weaviate.Client(\"http://localhost:8080\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset from disk\n",
    "Create some helper functions to create the dataset (20-newsgroup text posts) from disk. These methods are specific to the structure of your dataset, adjust accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "def get_post_filenames(limit_objects=100):\n",
    "    file_names = []\n",
    "    i=0\n",
    "    for root, dirs, files in os.walk(\"./data/20news-bydate-test\"):\n",
    "        for filename in files:\n",
    "            path = os.path.join(root, filename)\n",
    "            file_names += [path]\n",
    "        \n",
    "    random.shuffle(file_names)\n",
    "    limit_objects = min(len(file_names), limit_objects)\n",
    "      \n",
    "    file_names = file_names[:limit_objects]\n",
    "\n",
    "    return file_names\n",
    "\n",
    "def read_posts(filenames=[]):\n",
    "    posts = []\n",
    "    for filename in filenames:\n",
    "        f = open(filename, encoding=\"utf-8\", errors='ignore')\n",
    "        post = f.read()\n",
    "        \n",
    "        # strip the headers (the first occurrence of two newlines)\n",
    "        post = post[post.find('\\n\\n'):]\n",
    "        \n",
    "        # remove posts with less than 10 words to remove some of the noise\n",
    "        if len(post.split(' ')) < 10:\n",
    "               continue\n",
    "        \n",
    "        post = post.replace('\\n', ' ').replace('\\t', ' ')\n",
    "        if len(post) > 1000:\n",
    "            post = post[:1000]\n",
    "        posts += [post]\n",
    "\n",
    "    return posts       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize Dataset using BERT\n",
    "\n",
    "The following is a helper function to vectorize all posts (using our BERT transformer) which are entered as an array. The return array contains all the vectors in the same order. BERT is optimized to run on GPUs, if you're using CPUs this might take a while. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def text2vec(text):\n",
    "    tokens_pt = tokenizer(text, padding=True, truncation=True, max_length=500, add_special_tokens = True, return_tensors=\"pt\")\n",
    "    outputs = model(**tokens_pt)\n",
    "    tokens_pt.to('cuda') # remove if working without GPUs\n",
    "    return outputs[0].mean(0).mean(0).detach()\n",
    "\n",
    "def vectorize_posts(posts=[]):\n",
    "    post_vectors=[]\n",
    "    before=time.time()\n",
    "    for i, post in enumerate(posts):\n",
    "        vec=text2vec(sent_tokenize(post))\n",
    "        post_vectors += [vec]\n",
    "        if i % 25 == 0 and i != 0:\n",
    "            print(\"So far {} objects vectorized in {}s\".format(i, time.time()-before))\n",
    "    after=time.time()\n",
    "    \n",
    "    print(\"Vectorized {} items in {}s\".format(len(posts), after-before))\n",
    "    \n",
    "    return post_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run everything we have so far\n",
    "\n",
    "It is now time to run the functions we defined before. Let's load 50 random posts from disk, then vectorize them using BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Weaviate\n",
    "\n",
    "Now that we have vectors we can import both the posts and the vectors into Weaviate, so we can then search through them.\n",
    "\n",
    "### Init a simple schema\n",
    "Our schema is very simple, we just have one object class, the \"Post\". A post class has just a single property, which we call \"content\" and is of type \"text\".\n",
    "\n",
    "Each class in schema creates one index, so by running the below we tell weaviate to create one brand new vector index waiting for us to import data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weaviate_schema(client):\n",
    "    # a simple schema containing just a single class for our posts\n",
    "    schema = {\n",
    "        \"classes\": [{\n",
    "                \"class\": \"Post\",\n",
    "                \"vectorizer\": \"none\", # explicitly tell Weaviate not to vectorize anything, we are providing the vectors ourselves through our BERT model\n",
    "                \"properties\": [{\n",
    "                    \"name\": \"content\",\n",
    "                    \"dataType\": [\"text\"],\n",
    "                }]\n",
    "        }]\n",
    "    }\n",
    "\n",
    "    # cleanup from previous runs\n",
    "    client.schema.delete_all()\n",
    "\n",
    "    client.schema.create(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_posts_with_vectors(posts, vectors, client):\n",
    "    if len(posts) != len(vectors):\n",
    "        raise Exception(\"len of posts ({}) and vectors ({}) does not match\".format(len(posts), len(vectors)))\n",
    "        \n",
    "    for i, post in enumerate(posts):\n",
    "        try:\n",
    "            client.data_object.create(\n",
    "                data_object={\"content\": post},\n",
    "                class_name='Post',\n",
    "                vector=vectors[i]\n",
    "            )\n",
    "        except:\n",
    "            print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query=\"\", limit=3):\n",
    "    before = time.time()\n",
    "    vec = text2vec(query)\n",
    "    vec_took = time.time() - before\n",
    "\n",
    "    before = time.time()\n",
    "    near_vec = {\"vector\": vec}\n",
    "    res = client \\\n",
    "        .query.get(\"Post\", [\"content\", \"_additional {certainty}\"]) \\\n",
    "        .with_near_vector(near_vec) \\\n",
    "        .with_limit(limit) \\\n",
    "        .do()\n",
    "    search_took = time.time() - before\n",
    "\n",
    "    print(\"\\nQuery \\\"{}\\\" with {} results took {:.3f}s ({:.3f}s to vectorize and {:.3f}s to search)\" \\\n",
    "          .format(query, limit, vec_took+search_took, vec_took, search_took))\n",
    "    for post in res[\"data\"][\"Get\"][\"Post\"]:\n",
    "        print(\"{:.4f}: {}\".format(post[\"_additional\"][\"certainty\"], post[\"content\"]))\n",
    "        print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So far 25 objects vectorized in 17.370927810668945s\n",
      "So far 50 objects vectorized in 31.07236075401306s\n",
      "So far 75 objects vectorized in 49.37031865119934s\n",
      "Vectorized 100 items in 64.40030074119568s\n"
     ]
    }
   ],
   "source": [
    "init_weaviate_schema(client)\n",
    "posts = read_posts(get_post_filenames(100))\n",
    "vectors = vectorize_posts(posts)\n",
    "import_posts_with_vectors(posts, vectors, client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query \"the best camera lens\" with 1 results took 0.058s (0.034s to vectorize and 0.024s to search)\n",
      "0.8271:   Hi,  I have a HP LaserJet III for sale.  It's been printed for less than 1500 pages according the self test report.  I am asking $1000 for it.  If interested, please e-mail.  Thanks! \n",
      "---\n",
      "\n",
      "Query \"motorcycle trip\" with 1 results took 0.035s (0.025s to vectorize and 0.011s to search)\n",
      "0.8202:   Hi,  I have a HP LaserJet III for sale.  It's been printed for less than 1500 pages according the self test report.  I am asking $1000 for it.  If interested, please e-mail.  Thanks! \n",
      "---\n",
      "\n",
      "Query \"which software do i need to view jpeg files\" with 1 results took 0.055s (0.044s to vectorize and 0.011s to search)\n",
      "0.9191:   Is anybody using David Rapier's Hebrew Quiz software?  And can tell me how to *space* when typing in the Hebrew?  (space bar doesn't work, for me anyway...)  Email please; thanks.  Ken --  miner@kuhub.cc.ukans.edu opinions are my own      \n",
      "---\n",
      "\n",
      "Query \"windows vs mac\" with 1 results took 0.033s (0.023s to vectorize and 0.010s to search)\n",
      "0.8170:   > What's the difference between a 16550 UART and a 16550A UART? Thanks!  BUGS!!!!! 16550 (without the A) would sometimes get extra characters in the FIFO. This renders the FIFO useless.  Only get the 16550A. \n",
      "---\n"
     ]
    }
   ],
   "source": [
    "search(\"the best camera lens\", 1)\n",
    "search(\"motorcycle trip\", 1)\n",
    "search(\"which software do i need to view jpeg files\", 1)\n",
    "search(\"windows vs mac\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
