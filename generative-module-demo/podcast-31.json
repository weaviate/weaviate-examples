{
    "PodClip":[
    {
        "speaker": "Connor Shorten",
        "content": "Hey, everyone, I'm super excited to host Etienne Dilocker and Parker Duckworth for the Weaviate 1.17 release podcast. These releases are always so great. It feels like such a celebration of Weaviate and the hard work of the team to bring these new features into Weaviate. So today we're mainly talking about replication and hybrid search. And we're also welcoming Parker Duckworth for the first time on the Weaviate podcast. So we'll talk about Ref2Vec as well a little bit. So firstly, Parker, thank you so much for joining the Weaviate podcast. ",
        "podNumber": 31
    },
    {
        "speaker": "Parker Duckworth",
        "content": "Yeah, happy to be here. ",
        "podNumber": 31
    },
    {
        "speaker": "Connor Shorten",
        "content": "Awesome. I think this release is just so special because, you know, we all got together in Italy and these having everyone in the SeMI Technologies team in the same room as we're up on the whiteboard with the slides and presenting these new features. Etienne, can you tell us about your experience with that? ",
        "podNumber": 31
    },
    {
        "speaker": "Etienne Dilocker",
        "content": "Yeah, it's absolutely crazy. So we're a completely remote company, but being remote doesn't mean that you can't meet up occasionally. And this is exactly what we did. So we had 28, I think, or 27 or 28 folks in Italy in the same room. And we had a demo session planned for the last day of the week to show our progress. And I think by that point, it had been about a week that I had last synced up with Parker and Redouan who were working on the replication. And I was just sitting there and enjoying the demo. And I almost get a bit emotional looking at the kind of progress that we've made. Because I mean, like building a distributed database is a first for me. It's probably a first for most people that do that. And seeing that come together and sort of seeing the seed that we originally planted when it was a super tiny team and see the team grow and this feature come together. And I mean, it's like it's such a serious feature in the end, basically. Like when we started out thinking like, okay, once we have replication, that is really then we're in the big league, then we have these high availability and failover scenarios, these kind of things. So yeah, seeing that happen, absolutely unique experience. Can absolutely recommend it. Go to Italy with your company. ",
        "podNumber": 31
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, it's just an absolutely incredible experience. And yeah, at the end, at the last day, Parker and Redouan gave this incredible lecture about replication. So Parker, could you kind of take it away and tell us about replication? ",
        "podNumber": 31
    },
    {
        "speaker": "Parker Duckworth",
        "content": "Yeah, absolutely. So when I first started at the company, we had a milestone. It was like the last milestone of the roadmap, which was replication. And it seemed so far away, such a monumental task at the time, right? We didn't really have anything to support that at the time that I joined. So over time, we've built towards replication. For example, starting with backups, first, we introduced the ability to backup whatever's in your Weaviate instance, maybe on a single node. And then that evolved into distributed backups, which allows you to backup a cluster. And all of this was working towards the ability to automatically replicate or to support replication. And finally, we were able to build on the back of all the work that we had done with backups and introduce replication. So it's something that we had in mind the whole time throughout the planning and development of backups. We knew that we were building towards replication. So we wanted to just build it up incrementally until we got to this point. So the really fun and interesting thing is that really the capstone of the replication work, I guess you could say, was done in Italy. So up to that point, I hadn't met anyone in the team in person. I'm based in the US, and the rest of the core team is based in Europe and other places. And so getting to sit specifically next to Redouan, another core team member, and work with him in person to finalize this replication that we wanted to build was super, super interesting. And it just made the whole experience so great. So the way that we decided to implement replication, we first looked at the CAP theorem. Like, what tradeoffs do we want to make here? Do we want to prioritize consistency or availability or partition tolerance? And so after discussing many times with the core team and Redouan and I discussing for a while, we decided to make the tradeoff for partition tolerance and availability, similar to Cassandra. The thing with Weaviate is that it's super read heavy. So oftentimes the use case will be where we'll insert a large amount of data up front, and maybe there will be more inserts in the future. But we want to prioritize read availability. So that being said, we decided to follow a leaderless replication algorithm. So the idea is that a request will come in to a cluster of Weaviate nodes, and the node which happens to receive the request will be promoted, I guess you could say, as the coordinator for that request. And so this coordinator also considers itself one of the participant nodes or one of the other nodes that it needs to relay this replicated data to. So the coordinator will participate in a two-phase commit with the rest of the nodes, including itself. So a request comes in, let's say a write request for a piece of data, and then the coordinator receives the request and sends out a broadcast, basically asking every node that is part of the replica set, I guess you could say, to acknowledge the request has come in. Once that acknowledgment comes back, then the coordinator will actually send out the rest of the nodes, the data that it needs to actually commit or write to disk. So the advantage to going with this leaderless algorithm is that it's more flexible in the case of node failure. We don't have a single point of failure with a leader-follower algorithm. So yeah, it just makes things more flexible in the event of node outages and things like that. ",
        "podNumber": 31
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, that's so interesting to hear about. I love learning about these new database features in Weaviate. For example, when the backups came out, I had so much use out of that with my research on search features, because as we're building out this BEIR benchmarks, and we're going to talk about this later with hybrid search and our evaluation, MIRACL, and how we're evaluating this, these backups have been so useful. I can upload NF-Corpus, Arguana, SciFact, Trec-Covid, all these BEIR datasets, and then just back it up, restore it, run the tests, and then we know our hybrid search performance or whatever we're evaluating them. So can you help me understand further the use cases of replication, when people are going to need to use it? ",
        "podNumber": 31
    },
    {
        "speaker": "Etienne Dilocker",
        "content": "Yeah, so the biggest one is for reliability. So typically with replication, you want some kind of redundancy. So for example, if a node goes down, and this is a Parker already said that from the perspective of the CAP theorem, we're prioritizing availability and partition tolerance, and partition tolerance in a distributed cluster is basically a given. So partition tolerance in this case means a connection to another node could go down or the node itself could go down. And from the perspective of a node, the other node is down, it doesn't matter if it's actually down or if it just can't reach it. So kind of a partition tolerance is a given, and then availability would mean that use cases can still continue. And the cool thing with our replication is that the consistency level that the user wants is tunable. So some of that is going to be part of 1.17, some of it is only going to be part of 1.18. So we're rolling this out in phases, basically, but in phases that we believe make sense. So it's not that 1.17 is like a half finished feature, and then it's only complete in 1.18. But basically, what you get in 1.17 is complete and usable, and then you get new features that still fall under the umbrella of replication in 1.18. So yeah, with tunable consistency, you can basically say, how much priority do I give to the availability of reads versus writes? So with 1.17, every write is replicated to all nodes that participate in that particular class. So in consistency level terms, that's the replication level all. And then every read request that comes in, at least for searches, is done with the consistency level of one. So in other words, to write data into the cluster, all nodes need to be available. But to read data, any node could go down, basically, as long as you still have one. And that's a specific configuration. So in this kind of setup, this would work well, for example, for a search on an e-commerce application. So you would say, hey, products are updated only once a day, so we only need to be able to write data once every 24 hours. But users need to be able to search 24 hours. And if something goes down, they still need to be able to search. So that would be a great use case for the write with high consistency and read with low consistency kind of cases. But there are others. So you could, for example, say, if I write with a quorum and read with a quorum, then you could tolerate node failures on both cases. Or you could say, I both write and read with very little consistency, or minimal consistency, basically. And then you could tolerate a lot of node failures. But also, you could risk that data is out of sync. So basically, you have this eventual consistency kind of aspect, where you say, OK, for my use case, I can tolerate it if some data is not there at some point, but it needs to be there later on. So this is the high availability kind of use case, which, from my perspective, is the most requested reason, or the biggest motivator in replication. But there are others. So for example, what you can also do is you can use replication to scale your read throughput. So again, to stick with e-commerce, Black Friday. Let's say the kind of traffic that you expect on Black Friday is five times the kind of traffic that you would expect during a regular day or regular week. You don't want to provision your cluster for that peak load that you have one day of the year. You want to have your cluster sized appropriately for the rest of the year, and then scale it up basically just for that one or two or three day period. And this is something you can do with replication. So you could say, my replication factor is three for most of the year. So I have some redundancy. But maybe now I scaled up to five or eight or 10 or 15, because then I have 15 identical replicas that could serve the traffic. And basically, this scales linearly at least. So 15 nodes could serve five times the traffic that three nodes could serve. And finally, there's another one. And this is sort of more on the roadmap. But that's the multi-data center kind of replication. So for regional proximity, so you could have two places on Earth. So maybe I think we've used that before in HNSW graph examples. So let's stick with that. We have Frankfurt, Germany, and I don't know, Boston, USA. So if you had a data center somewhere in the middle, it's kind of a bad example right now, because we're in the middle of the Atlantic. But that's OK. Let's assume there was a data center in the Atlantic. Then the kind of latencies that users would get would be pretty much the same for the users in Boston and for the users in Frankfurt, because they have the same sort of regional distance to the data center. But no one has a real good latency, because no one is close to the data center. So we could say, let's move the data center to Boston. Super for the Boston users, not so great for the Frankfurt users. But now with multi-DC replication, what you could also do is you could have a data center in Boston. You can have a data center in Frankfurt. And each of those users contacts the data center that's close to them, so they have good latencies. And then, of course, the data centers need to stay in sync as well. And this is basically the multi-data center replication. This is something that's not yet present in 1.17, also not going to be part of 1.18 yet. But it's something that the kind of architecture that we've chosen in Weaviate, that is supported by the architecture. So this is something that if you want to have it, I think we have a feature request to get for it on GitHub already. So upload it, and then we'll build it. ",
        "podNumber": 31
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, I love how you gave the example of kind of the read-write trade-offs with consistency levels when you want to use each thing. I think that's always a super important thing to understand. The distributed systems, it's so interesting to learn about. It reminds me of our podcast. People are out for a binge on Weaviate Podcast. We did another podcast with Erik Bernhardsson on running vector search in production. And it reminds me of that topic of what it takes to scale out your e-commerce store so you can handle Black Friday. I think in general, it's so interesting. So also quickly, I want to touch on this iterative release, and Etienne, you've recently written a very popular blog post on product engineering. I think now would be a great time to kind of touch on product engineering and your thinking around these iterative releases and the general kind of philosophy behind how you think about these things. ",
        "podNumber": 31
    },
    {
        "speaker": "Etienne Dilocker",
        "content": "Yeah. So for those of you who haven't heard of product engineering yet, it's kind of the merger of product decision and engineering decisions. So it's kind of blurring the lines between traditionally you might have a product department or a product team, and then you have the engineering department and these parties hate each other and they don't talk to each other and don't collaborate. In product engineering, the idea is that you soften these boundaries and you have collaboration. So in a startup, to me, this makes a lot of sense because you have small teams. And if you have a team with like three developers, you can't afford to have a dedicated product manager and maybe a dedicated lead developer, and then who's going to do the work? Basically, they have like a manager to engineer ratio that just doesn't make sense. So something that naturally involves in those kinds of settings is that, yeah, you blur the lines and you maybe have an engineer who takes over a couple of product responsibilities, or maybe you have a product manager who has an engineering background and can do the kind of prototyping themselves. And then, yeah, you have easier communication, more collaboration and have something that I would say, yeah, sort of is more productive and feels more natural and you don't have these kind of artificial boundaries between the two. For our replication release, I mentioned that before already, we're really, really interested in the feedback that we're getting and we're really trying to provide value early, but value, but really value, like not just sort of give you something half finished. So like, hey, we went for a minimal implementation so that you could have it sooner, but don't use it because it's not really meant for production. And that's kind of not what we're trying to do, but instead we're trying to say like, okay, what is the combination of features that you would use or what you would need to use it in production? And then maybe this only works for 80% of use cases, maybe it doesn't work for the remaining 20% yet. That's fine because the remaining 20% they'll be covered later on, but that's no reason for us to say to the first 80%, hey, you're going to have to wait because we're only going to release it once we have 100% covered. So that's kind of the idea of doing that in iterations. And of course we get the feedback. Once something is out, it gives us the ability to still pivot. And for this to sort of tie this back into product engineering, it's super important to have that feedback cycle and not have artificial steps in between where someone has to relay a message from one department to another department. And in the end you have engineers that never talk to users. It's the complete opposite basically. Everyone in the core team or in other teams can be a member of our, or not can be a member, but typically is a member of our public Slack. So they communicate with engineers right away. And then if new ideas pop up through that, we discuss the ideas internally and it's not like, well, no, the product manager said we're not going to do that, but it's way more collaborative. ",
        "podNumber": 31
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, incredible. And I think this is a great transition to start talking about hybrid search and our philosophy and the overall how we've developed it and so on. And so maybe let me set the stage by describing what hybrid search is. So hybrid search describes combining keyword scoring methods with vector search methods. So I think we're all pretty familiar with the vector search part. That's where we encode data with machine learning models, build up a vector index and search for the approximate nearest neighbors and that. But now let's kind of focus a little more on the keyword scoring methods. So sort of the foundational algorithm is TF-IDF, term frequency, inverse document frequency, where you score some sentence like, I'm super excited to welcome Etienne and Parker Duckworth to the podcast based on the kind of uniqueness of these terms in that query with how unique it is to the collection of documents that I have. So then going from TF-IDF to BM25, BM25 introduces this binary independence model. You don't count how many times the keywords can appear in the document, just whether it appears or not. You similarly normalize it for the length of the document. So it's a bit of a modification to TF-IDF. And it's another way of scoring these documents based on the keywords that has been really successful. So then we have these two search algorithms. And so now with hybrid, we're combining the results from each of the lists. So we're going to dive a little further into the rank fusion and then also say BM25 and the extension to BM25F. But I want to come back to this product engineering. And Etienne, I want to ask about, I thought with this project, you did such a great job of leading the team. This was one of the projects that I've been a part of since joining Weaviate, where there's been like, it's like a task force almost like, this is our project, this is your responsibility, this is your responsibility, and then we've just kind of come together and it's almost finished and it's so exciting. So can you tell me about like, your initial thinking around the development of the hybrid search project?",
        "podNumber": 31
    },
    {
        "speaker": "Etienne Dilocker",
        "content": "Yeah, that's great to hear, by the way. That's really nice. Yeah, so for our listeners to understand a bit sort of how we structure that internally, we have the core team itself, which basically builds Weaviate, which is kind of a lot of what we do, but by far not the only thing that we do. And then Connor is part of the research team as well. So besides like the podcast and other sort of DevRel activities, there's also the research part. And what we consider research and research that the term research, depending on what your background is, this can have very different definitions, or it can have very different meanings. But we use research in the sense that we say we've identified an opportunity somewhere, something that we will most likely want to add to Weaviate. But there is some kind of a question that we need to answer first. And this question could be something as simple as what is the best UX to integrate this into our APIs? Like how would our users want to use it? Like, do we want to give the user a lot of control? Or do we want to maybe abstract something? So this could be a question. It could be a question of how are we going to build it? So especially in... So you mentioned Rank Fusion and Score Based Fusion. And these terms, this is basically something that you know way better than I do, and something where we benefit so much from having these kind of collaborations within the company. So this sort of how do we build it? What do we need? What do we need to figure out how to be able to build it? Could be an evaluation also, something like, does this idea make sense? Like it looks good on paper, but what happens if we try it at scale? Let's try it with 10K objects, a million objects, maybe a billion objects. Does it scale? Does it fit into Weaviate in that sense? And this is something where Hybrid, I think early on, we identified that there is an opportunity. And so like, okay, let's get started. Let's see what it is. Let's see what do we need? Because Hybrid sort of in a sense, you need both the building blocks for Hybrid, both the BM25 search and the vector search. You need to have both. Vector search obviously is kind of what Weaviate is about. So we can safely assume that we have vector search covered. BM25 is something that we gradually started building. It was actually TF-IDF in sort of the simple building block for BM25. But I think from the indexing perspective, it's actually the same or it's like one or two parameters need to additionally be indexed for BM25. That is something that we actually had in mind in the very, very first prototype that we built. So we didn't have any APIs for TF-IDF or BM25, but we had the inverted index early on, I think over two years ago, we added the inverted index to Weaviate. And it already had this, and Parker, you may have come across that in the code. Whenever we put those buckets, we had like buckets for with frequency and without frequency. The word frequency is basically for all the text properties. We additionally to indexing the word, we also index the frequency. And that was in preparation for that whole TF-IDF BM25 step. So we kind of knew that it might be something that we want to add at some point. But we also have to figure out like what is the real value of it? And if I'm 100% honest, something that I don't know at this moment is, will hybrid search play a role five or 10 years from now? It could easily be the case. And I don't think anyone can confidently answer that. It could be the case that semantic search just keeps improving so much that hybrid search basically is more of a stopgap solution at the moment to bridge a gap and the gap being exact keyword match in out of domain search. Or it could be that while it still improves, hybrid search is just always going to be better because it's the combination of two things. And this is something that, yeah, I don't know. And I don't think anyone really knows. It's something that I find super exciting. And yeah, quote me on this five years from now. And let's see. Let's see how it turned out. ",
        "podNumber": 31
    },
    {
        "speaker": "Parker Duckworth",
        "content": "There seems to have been quite a bit of buzz about hybrid search in the community as well. I think in our Slack community, I often see people requesting this feature or asking when it's going to be available or being excited when they hear that it's going to be available soon. ",
        "podNumber": 31
    },
    {
        "speaker": "Connor Shorten",
        "content": "And I think, yeah, it's always awesome to see people asking for things and then have it delivered. That's so cool. And I think so in preparation for Weaviate Air, Erika and I were coming up with a demo and I think this example of how to catch an Alaskan pollock, that query is a great example where you have the semantic meaning of catch. You don't mean catch a baseball, catch a cold. You mean fishing. And then Alaskan pollock, which is a specific keyword and that kind of fusion. I think what you're saying, Etienne, is very interesting about will the vectors be able to contain that kind of keyword centric focus in the future? And I kind of think so also, especially with like, say, the way that ColBERT would re-rank with token representations. But I think in addition to that, this rank fusion thing that we've been exploring will be very useful. I can imagine combining it with the where filter and near text search to have this kind of boost. So, you know, say in recommendation or like you're in an e-commerce store again, it's Black Friday and you go for rugs and it's like, you know, $2,000 for a rug, $3,000 for a rug. And so you also want to have where price is less than $300, but then you want to have like the fusion where you also show the extravagant for rugs so you can have that kind of rank fusion. So I think that rank fusion is a core primitive of search pipelines that we've explored in this particular feature. One other thing, and then I really want to dive into ref2vec with Parker and is this how we've been benchmarking hybrid search and it comes back to the backups. And I think this is just so exciting for the development of Weaviate and our features and our connection to the science is we've been uploading the BEIR benchmarks to Weaviate and we have the NDCG, the recall scores. And I think it's just an incredible, exciting step for us. So I'm kind of curious, Etienne, if I could ask this kind of question about like your thoughts on sort of the BEIR benchmarks and just sort of these kind of like academic information retrieval metrics and how they play with feature development. ",
        "podNumber": 31
    },
    {
        "speaker": "Etienne Dilocker",
        "content": "Yeah, so the BEIR benchmark is definitely more your area of expertise than mine. But I think this is exactly what makes this so great that we now have something quantifiable as well as opposed to just sort of... Benchmarks are always reflective of some sort of a scenario. So you could set up a benchmark in a way to produce some kind of a result. So let's say the benchmark is primarily keyword based, then probably a keyword based algorithm is going to be better on it. Let's say it doesn't care so much about specific unknown words, but it matches a domain of a semantic search based model, then you're probably skewing it towards that. So benchmarks are... Yeah, or you always need the asterisk for what is the benchmark meant to show. But nevertheless, I mean, that's not a reason not to use benchmarks. It's very good to be able to objectively say, okay, A is better than B. Whether that matches to A being better than B for a specific use case, that is something that users have to see for themselves. So that is why it's super important to me to have both approaches, like the quantifiable approach, but also the qualitative approach where it's like, okay, an actual use case. And we have our customer success team who deals with the paid cases that we have for SeMI. We have the open source community who sometimes share data or give us some insight into what's working for them. So the mix of both to me is super important that we don't just make these claims of cherry picked examples, but that we also don't do the opposite of saying like, hey, nothing is cherry picked, everything is scientific. But then the user comes in saying like, why doesn't it work for me? ",
        "podNumber": 31
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, it's absolutely fascinating, especially with kind of coming into the trending topic of the day, chatGPT, I don't mean to distract too much, but this kind of ad hoc evaluation, I did one query, I like the result. That means the system works. Yeah. ",
        "podNumber": 31
    },
    {
        "speaker": "Etienne Dilocker",
        "content": "And that goes both ways, right? Like you see the people saying like, hey, it's the best thing ever. I only have positive results. And you see people saying it's the worst thing ever. I only have negative examples. ",
        "podNumber": 31
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, exactly. And I think it's worth kind of knowing that these systems are a little different than maybe traditional software cases where these edge cases, like machine learning performance is very like long tailed, like hit or miss. And I think the BEIR benchmarks, a particular reason why I'm so excited about this particular work is the diversity captured in it. It has papers about COVID-19, it has financial questions like, are my personal taxes separate from my hobby income? And then you have like nutrition questions about like, are multivitamins a waste of money? So you have this incredible diversity, the diversity in query length. And I think we're also seeing the kind of intents, this kind of exploration is, this research is emerging as well, where you'd say, what is the intent of the search task and that kind of exploration? So yeah, overall, I just couldn't be more excited about the benchmarking. I think it's such an exciting step for us. So I want to pivot topics. I'm so excited to have Parker, especially because he was so pivotal to the development of Ref2Vec. Parker, could you start by kind of explaining what Ref2Vec is? And then I really want to dive into sort of some of the questions that we've been seeing in our community chat, like particularly clarifying, updating the references and how this kind of cascades backwards, thinking around like, can we have custom aggregation functions, but maybe we could set the stage. Could you tell us about what Ref2Vec is? ",
        "podNumber": 31
    },
    {
        "speaker": "Parker Duckworth",
        "content": "Yeah, certainly. So Ref2Vec Centroid is a new module that we released recently. And the idea of it is that an object which is set to be vectorized, so to speak, by Ref2Vec Centroid, a vector isn't produced by this object itself, but it's produced by like the aggregate of its referenced vectors. So the Ref2Vec module will take an object and then grab the vectors from all of its references, all of its referenced objects. And then we'll compute a centroid with that set of vectors to find something that's similar to all of these things at once. And so the idea is that this is really useful when you want to represent something as an aggregation of other things, right? For example, users based on their likes, right? What can we show to a user that is something that aligns with what their express interests are in Ref2Vec Centroid is something that's perfect for doing something like that. ",
        "podNumber": 31
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, exactly. I think the sort of the most obvious use case, I think, is the kind of bipartite graph recommendation case where you have users, products, the user liked a few products, and now you represent the user with the vector from the products. And then that's the query vector to recommend new products. I think that really helps just get the idea quickly. Yeah, I think I want to kind of stay on this topic of graphs and Weaviate a little more. I have sort of my story of coming to Weaviate is, you know, when I first talked to Bobby, we talked about how he was really interested in the semantic web and ontologies. And then sort of shortly after we had that podcast, I went to New York to meet Laura at the Knowledge Graph Conference where there are, you know, companies like Tiger Graph, Relational AI, where they have these tuples. So I always kind of had this thinking that like Weaviate is going to have this focus on the graphs. Sort of opening this up, and maybe, Etienne, you could start, can you tell me about kind of the motivation behind this cross-reference design? Because I think it's so powerful, so under, I don't want to say under-appreciated, but I think it's maybe not hyped enough, like this kind of design of having cross-references, the way it lets you do multi-vector, the way it lets you do multi-modal. I think it's such a powerful part of the data schema design in Weaviate. ",
        "podNumber": 31
    },
    {
        "speaker": "Etienne Dilocker",
        "content": "Yeah, this goes back a bit to the history of Weaviate because there was a phase before we called ourselves, or before we called Weaviate a vector search engine, because we were sort of trying to figure out like, what is it that Weaviate can add or where it can add the kind of value? And at that point, Weaviate was no database on its own yet, but Weaviate was sort of thought as a layer on top of other databases. And at that point, we were actually planning on running Weaviate on top of graph databases. So we had an integration for what's called JanusGraph, a tool that I had not heard before and also kind of haven't heard of since, but I think it's like, it's a super niche tool, super good at what it does, but also like a relatively small niche. And the idea of JanusGraph was that you build this itself on top of other databases. So I think at that point, and I don't know if this is still true. It was Cassandra and Elasticsearch, I think, so sort of like stored data in Cassandra and then a query using Elasticsearch. And this enabled you to sort of build this like super large scale graphs. And then Weaviate was basically the AI layer on top of that. And originally, the idea was before we started, yeah, basically accepting vectors together with objects to only vectorize the schema. So one of the very first use cases was you have these knowledge graphs, and they have different ontologies. So you would have a graph here and a graph there, and you kind of know that there's an overlap. But because people didn't use the exact same words, it was super difficult to really match like what is like in these two graphs, like, yeah, they do intersect, but you don't know how. So the original idea was to use, yeah, basically NLP technology, early NLP technology of the time to just figure out what the right schema is. And then at some point, Bob and I were on a call, and this was super early on, I think that the team was very, very small. And we were kind of figuring out this idea, like, what if we tried that same approach, not on the schema, but in the actual data? And we were both like, nah, that's crazy. We can't do that. And then we tried it out and was like, whoa, this works kind of well. And that was kind of the step where sort of this semantic graph ontology tool turned into a vector database. So we kind of pivoted completely. And that was also the point when we started. I don't want to say rewrite Weaviate, because some parts of it, like the GraphQL API, for example, still is modeled after that original structure. But yeah, it was kind of sort of shifting the focus a bit. But at the same time, our early users already had graphs that they represented with Weaviate. So they're like, okay, well, we can't just abandon them. We can just say like, okay, Weaviate now switched from sort of this semantic graph tool to a NoSQL document only search engine. And now you can't represent your graphs anymore. They're like, okay, Weaviate is probably not going to become a graph database, because in architectural architecture, it's always trade-offs. And like, what do you prioritize? We said, okay, search is more important for us than craft traversal. So we kind of skewed the architecture towards search. So the HNSW index and the inverted index and the way it distributes data across nodes and these kind of things, these are all set up for search. But we said we want to keep the cross-references. And the cross-references, from an architectural perspective, they're basically just links. And of course, there's a couple of optimizations you can do when you resolve those links. But yeah, that's kind of the history of why they're there. And now sort of it's an enabler for new use cases, basically. ",
        "podNumber": 31
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, that's amazing. I've always wondered, like the vectorized class name thing now makes so much more sense to me with the context of that. And that's so interesting. So I really want to dive into the technical details. Parker, could you tell us about like, because we're seeing this question about kind of people want to understand exactly how if they have A to B to C, and they update C, will it cascade back? Like, this kind of question, it seems to be something that people are curious about. ",
        "podNumber": 31
    },
    {
        "speaker": "Parker Duckworth",
        "content": "Yeah, absolutely. So currently, the only way to update an object's reference vector is by updating that object itself, the parent object, which holds the references. So let's say object A references object B and C, and object A's vector, reference vector, is the centroid of B and C's vectors. If B or C are updated, A's reference vector does not change. Right now, we don't have any sort of back channel mechanism that allows that information to reach the object which references those vectors. And primarily, it was because this is our first iteration. And this is something that could be very computationally heavy if, for example, we have tons of these reference vectors around. So currently, the only way to update an object's reference vector is to update that object's set of references directly. So that can be done either just by posting an entirely new object, or I guess you could say putting a new object with the same ID and a new set of reference vectors, or deleting some references from that object, or updating that object's references one at a time. But basically, the only way to update an object's reference vector is to mutate that object's set of references on itself. Whereas updating one of its references directly is not going to affect that parent object's reference vector. ",
        "podNumber": 31
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah. And that idea of the kind of, yeah, when you really chain out these graphs, and there's kind of like the bipartite graph I originally described where you can have multiple edges. You maybe also have it going back and forth, kind of, if you imagine data like that. But like multiple classes chained together, I think the aggregate functions are going to be, that's going to be something that we can really explore. And as Etienne said, in laying the future for what we can do. Maybe I even want to just work this in there because I'm happy to have gone. So last night, I went to MIT's Learning on Graphs conference. And it was, firstly, yeah, it was super cool to be at MIT. It's super, super smart people. And just walking around, you're like, nice, I'm at MIT. But seeing the research and seeing the thinking around graph neural networks, it can be so intense about this kind of thing of what kind of problems can deep learning broadly solve, sort of connecting to the Turing machines and what problems can be solved, like NP completeness, can graph neural networks take that on. But I think there is a big middle ground for just a graph convolutional network being used somewhere that's useful. And I think just this basic idea of chaining these things together, aggregating, maybe that could be the use of that. And we could similarly have a Python app for our module inference, the same way we have text2vec, all these things, we have that kind of container for the PyTorch geometric library. So kind of pivoting in, I know that the graph neural network aggregation thing is a bit intense, but can we talk about what would it take to build in custom aggregation functions, maybe starting with just having, biasing it so that the mean centroid is most heavily impacted by the most recently added cross-reference? ",
        "podNumber": 31
    },
    {
        "speaker": "Parker Duckworth",
        "content": "Yeah, currently our only module in the class of ref2vec is ref2vec-centroid. This was built purposely to be able to be easily expanded into more centroid type algorithms, or more algorithms to calculate this reference vector however you want to calculate it. So Weaving's module system is by design very modular. So if we were to want to introduce something like this, most of the boilerplate, I guess you could say the groundwork has already been set. So it's just a matter of coming up with the way you want to calculate these reference vectors, and then introducing a new module which piggybacks this existing ref2vec framework that we've built within the Weaving module system to use this new algorithm to calculate the reference vector. So I would say for any reference or ref2vec centroid modules in the future, it's not a whole lot of work to introduce a new one. It's just a matter of figuring out how you want to calculate these reference vectors. ",
        "podNumber": 31
    },
    {
        "speaker": "Etienne Dilocker",
        "content": "Yeah, I'm just thinking out loud about the recency bias. Because I think, and I'm not 100% sure, but basically references in Weaviate have a specific order. We don't ever make use of the fact that they have an order, but on disk, they're saved in order. So most likely we could use that fact. We don't have timestamps for a reference. So right now we couldn't easily say something like, okay, from one to two was a very short time difference, and then from two to three, it was a large one. But at least we know the order. So if we just want to give the most weight to the most recent one, it would probably be as simple as giving the most weight to the last element of the array. ",
        "podNumber": 31
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, super interesting. I think one other thing that excites me, and yeah, I think that the building blocks of that are in place, and that will be super impactful just based on that a little longer. If you imagine you want to have recommendation without logging in and having that long archive of user data, you want to just be able to scroll through TikTok and quickly have recommendations. I think that kind of thing lets you control it by giving the signal of recency. One other thing that excites me is this idea of clustering the embeddings. I think that could be super powerful, especially for diverse interests. So if you've liked these products and it's Nike shoes, Adidas shoes, Jordan shoes, I think instead of averaging it, we could have this clustering, and then the centroids could be used, which brings this topic of how might we represent centroids. I think the cross-reference thing, again, is we would use it again to do multi-vector representation and that kind of idea. So super cool. I think, yeah, this overall, this 1.17, and thanks so much for the discussion on ref2vec. I'm so excited about ref2vec. I think this kind of graph structure, how we can send embeddings through the graphs and aggregate them, I think a lot of people are excited about it because I think it's exciting. But anyways, thanks so much. I think, yeah, replication, hybrid search, and sort of the Italy 1.17 release, all of it. ",
        "podNumber": 31
    },
    {
        "speaker": "Etienne Dilocker",
        "content": "Yeah. A few smaller improvements as well. We'll mention them in our release blog post, but a couple of performance improvements regarding startup time. So both for startup times of the time that the application takes to restart. So for example, if there was a node failure, and this is something that with replication we have a lot more tolerance for, but even with replication, you still care about the time that the node is back because there may be a time window when it's unavailable. So there are a lot of improvements around the startup time, but also, and this was sort of a similar cost for this, we've also improved batch latencies, and they're particularly sort of the P999 or the max latency. So we had a pretty constant write speed already based on the LSM store having a constant write speed, but then we had like these occasional peaks and that those could lead to timeouts and then timeout would lead to a retry, and then the retry would put more load on the cluster and all these kind of chain of events. And we have a lot of fixes around those as well that we implemented in 1.17, which is sort of one of these for me, my favorite category, like not a very exciting feature, but super exciting for those that actually operate Weaviate clusters. ",
        "podNumber": 31
    },
    {
        "speaker": "Connor Shorten",
        "content": "Awesome. Well, thanks again both so much for coming on the podcast and everyone, please check out Weaviate 1.17 and thank you so much again for listening. ",
        "podNumber": 31
    },
    {
        "speaker": "Parker Duckworth",
        "content": "Awesome. Thanks, Connor.",
        "podNumber": 31
    }
]}